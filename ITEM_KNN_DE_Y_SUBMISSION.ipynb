{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15e64662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d37ca4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sessions_train.csv\")\n",
    "\n",
    "df = df[df[\"locale\"]==\"DE\"].drop(columns={\"locale\"}).reset_index(drop=True)\n",
    "\n",
    "# https://github.com/rn5l/session-rec/blob/master/algorithms/knn/iknn.py\n",
    "class ItemKNN:\n",
    "    '''\n",
    "    ItemKNN(n_sims = 100, lmbd = 20, alpha = 0.5, session_key = 'SessionId', item_key = 'ItemId', time_key = 'Time')\n",
    "    Item-to-item predictor that computes the the similarity to all items to the given item.\n",
    "    Similarity of two items is given by:\n",
    "    .. math::\n",
    "        s_{i,j}=\\sum_{s}I\\{(s,i)\\in D & (s,j)\\in D\\} / (supp_i+\\\\lambda)^{\\\\alpha}(supp_j+\\\\lambda)^{1-\\\\alpha}\n",
    "    Parameters\n",
    "    --------\n",
    "    n_sims : int\n",
    "        Only give back non-zero scores to the N most similar items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100)\n",
    "    lmbd : float\n",
    "        Regularization. Discounts the similarity of rare items (incidental co-occurrences). (Default value: 20)\n",
    "    alpha : float\n",
    "        Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules).\n",
    "    session_key : string\n",
    "        header of the session ID column in the input file (default: 'SessionId')\n",
    "    item_key : string\n",
    "        header of the item ID column in the input file (default: 'ItemId')\n",
    "    time_key : string\n",
    "        header of the timestamp column in the input file (default: 'Time')\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_sims=100, lmbd=20, alpha=0.5, session_key='SessionId', item_key='ItemId', time_key='Time'):\n",
    "        self.n_sims = n_sims\n",
    "        self.lmbd = lmbd\n",
    "        self.alpha = alpha\n",
    "        self.item_key = item_key\n",
    "        self.session_key = session_key\n",
    "        self.time_key = time_key\n",
    "\n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "        '''\n",
    "        data.set_index(np.arange(len(data)), inplace=True)\n",
    "        itemids = data[self.item_key].unique()\n",
    "        n_items = len(itemids)\n",
    "        data = pd.merge(data, pd.DataFrame({self.item_key: itemids, 'ItemIdx': np.arange(len(itemids))}),\n",
    "                        on=self.item_key, how='inner')\n",
    "        sessionids = data[self.session_key].unique()\n",
    "        data = pd.merge(data, pd.DataFrame({self.session_key: sessionids, 'SessionIdx': np.arange(len(sessionids))}),\n",
    "                        on=self.session_key, how='inner')\n",
    "        supp = data.groupby('SessionIdx').size()\n",
    "        session_offsets = np.zeros(len(supp) + 1, dtype=np.int32)\n",
    "        session_offsets[1:] = supp.cumsum()\n",
    "        index_by_sessions = data.sort_values(['SessionIdx', self.time_key]).index.values\n",
    "        supp = data.groupby('ItemIdx').size()\n",
    "        item_offsets = np.zeros(n_items + 1, dtype=np.int32)\n",
    "        item_offsets[1:] = supp.cumsum()\n",
    "        index_by_items = data.sort_values(['ItemIdx', self.time_key]).index.values\n",
    "        self.sims = dict()\n",
    "        for i in range(n_items):\n",
    "            iarray = np.zeros(n_items)\n",
    "            start = item_offsets[i]\n",
    "            end = item_offsets[i + 1]\n",
    "            for e in index_by_items[start:end]:\n",
    "                uidx = data.SessionIdx.values[e]\n",
    "                ustart = session_offsets[uidx]\n",
    "                uend = session_offsets[uidx + 1]\n",
    "                user_events = index_by_sessions[ustart:uend]\n",
    "                iarray[data.ItemIdx.values[user_events]] += 1\n",
    "            iarray[i] = 0\n",
    "            norm = np.power((supp[i] + self.lmbd), self.alpha) * np.power((supp.values + self.lmbd), (1.0 - self.alpha))\n",
    "            norm[norm == 0] = 1\n",
    "            iarray = iarray / norm\n",
    "            indices = np.argsort(iarray)[-1:-1 - self.n_sims:-1]\n",
    "            self.sims[itemids[i]] = pd.Series(data=iarray[indices], index=itemids[indices])\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids, skip=False, mode_type='view',\n",
    "                     timestamp=0):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        '''\n",
    "        preds = np.zeros(len(predict_for_item_ids))\n",
    "        sim_list = self.sims[input_item_id]\n",
    "        mask = np.in1d(predict_for_item_ids, sim_list.index)\n",
    "        preds[mask] = sim_list[predict_for_item_ids[mask]]\n",
    "        return pd.Series(data=preds, index=predict_for_item_ids)\n",
    "\n",
    "\n",
    "# Funci√≥n para reorganizar el DataFrame\n",
    "def reformat_dataframe(df):\n",
    "\n",
    "    # Convertir las cadenas en matrices de NumPy\n",
    "    df[\"prev_items\"] = df[\"prev_items\"].apply(lambda x: re.findall(r\"'([^']*)'\", x))\n",
    "    session_id_col = []\n",
    "    item_id_col = []\n",
    "    time_col = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        session_id = i\n",
    "        for j, item_id in enumerate(row[\"prev_items\"]):\n",
    "            session_id_col.append(session_id)\n",
    "            item_id_col.append(item_id)\n",
    "            time_col.append(j)\n",
    "\n",
    "        session_id_col.append(session_id)\n",
    "        item_id_col.append(row[\"next_item\"])\n",
    "        time_col.append(len(row[\"prev_items\"]))\n",
    "\n",
    "    new_df = pd.DataFrame({\"SessionId\": session_id_col, \"ItemId\": item_id_col, \"Time\": time_col})\n",
    "    return new_df\n",
    "\n",
    "# Reorganizar el DataFrame\n",
    "new_data = reformat_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "090907be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B09W9FND7K, B09JSPLN1M]</td>\n",
       "      <td>B09M7GY217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B076THCGSG, B007MO8IME, B08MF65MLV, B001B4TKA0]</td>\n",
       "      <td>B001B4THSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[B0B1LGXWDS, B00AZYORS2, B0B1LGXWDS, B00AZYORS...</td>\n",
       "      <td>B0767DTG2Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[B09XMTWDVT, B0B4MZZ8MB, B0B7HZ2GWX, B09XMTWDV...</td>\n",
       "      <td>B0B4R9NN4B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[B09Y5CSL3T, B09Y5DPTXN, B09FKD61R8]</td>\n",
       "      <td>B0BGVBKWGZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111411</th>\n",
       "      <td>[B06X9BB2D7, B09RWWGXZJ]</td>\n",
       "      <td>B09RWWWYGZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111412</th>\n",
       "      <td>[B0BK2WGCN4, B08H93ZRLL, B0BK2WGCN4, B08H93ZRLL]</td>\n",
       "      <td>B001BWJEXK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111413</th>\n",
       "      <td>[B08X1SDBLB, B06WVZRBJ1]</td>\n",
       "      <td>B09YRTCM8X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111414</th>\n",
       "      <td>[B0B8NNHR5N, B0BBCCB2S1, B09R222SDP, B0B5FBHX8...</td>\n",
       "      <td>B07XJ3H1RM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111415</th>\n",
       "      <td>[B09Y5SSN7R, 3731861860]</td>\n",
       "      <td>B0B87CNH9C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1111416 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prev_items   next_item\n",
       "0                                 [B09W9FND7K, B09JSPLN1M]  B09M7GY217\n",
       "1         [B076THCGSG, B007MO8IME, B08MF65MLV, B001B4TKA0]  B001B4THSA\n",
       "2        [B0B1LGXWDS, B00AZYORS2, B0B1LGXWDS, B00AZYORS...  B0767DTG2Q\n",
       "3        [B09XMTWDVT, B0B4MZZ8MB, B0B7HZ2GWX, B09XMTWDV...  B0B4R9NN4B\n",
       "4                     [B09Y5CSL3T, B09Y5DPTXN, B09FKD61R8]  B0BGVBKWGZ\n",
       "...                                                    ...         ...\n",
       "1111411                           [B06X9BB2D7, B09RWWGXZJ]  B09RWWWYGZ\n",
       "1111412   [B0BK2WGCN4, B08H93ZRLL, B0BK2WGCN4, B08H93ZRLL]  B001BWJEXK\n",
       "1111413                           [B08X1SDBLB, B06WVZRBJ1]  B09YRTCM8X\n",
       "1111414  [B0B8NNHR5N, B0BBCCB2S1, B09R222SDP, B0B5FBHX8...  B07XJ3H1RM\n",
       "1111415                           [B09Y5SSN7R, 3731861860]  B0B87CNH9C\n",
       "\n",
       "[1111416 rows x 2 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8257571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B09W9FND7K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>B09JSPLN1M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>B09M7GY217</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>B076THCGSG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>B007MO8IME</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948394</th>\n",
       "      <td>1111414</td>\n",
       "      <td>3750524505</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948395</th>\n",
       "      <td>1111414</td>\n",
       "      <td>B07XJ3H1RM</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948396</th>\n",
       "      <td>1111415</td>\n",
       "      <td>B09Y5SSN7R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948397</th>\n",
       "      <td>1111415</td>\n",
       "      <td>3731861860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948398</th>\n",
       "      <td>1111415</td>\n",
       "      <td>B0B87CNH9C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5948399 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SessionId      ItemId  Time\n",
       "0                0  B09W9FND7K     0\n",
       "1                0  B09JSPLN1M     1\n",
       "2                0  B09M7GY217     2\n",
       "3                1  B076THCGSG     0\n",
       "4                1  B007MO8IME     1\n",
       "...            ...         ...   ...\n",
       "5948394    1111414  3750524505     8\n",
       "5948395    1111414  B07XJ3H1RM     9\n",
       "5948396    1111415  B09Y5SSN7R     0\n",
       "5948397    1111415  3731861860     1\n",
       "5948398    1111415  B0B87CNH9C     2\n",
       "\n",
       "[5948399 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cb9dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo ItemKNN\n",
    "iknn = ItemKNN(n_sims=100, lmbd=20, alpha=0.5, session_key=\"SessionId\",\n",
    "               item_key=\"ItemId\", time_key=\"Time\")\n",
    "iknn.fit(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b2cfb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "# create a binary pickle file \n",
    "#f = open(\"iknn.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "#pickle.dump(iknn.sims,f)\n",
    "\n",
    "# close file\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bf7edfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"iknn.pkl\",'rb')\n",
    "iknn2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5f5588cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_dic = iknn.sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a8f54a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "segundo_dic = {}\n",
    "\n",
    "for clave, valores in primer_dic.items():\n",
    "    segundo_dic[clave] = list(valores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d9fa056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "objetos = new_data.ItemId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2227f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "caf1ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(x):\n",
    "    x = x.replace('[', '').replace(']', '').replace(\"'\", '').replace('\\n', ' ').replace('\\r', ' ')\n",
    "    l = [i for i in x.split() if i]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "024d8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test[test[\"locale\"].isin([\"DE\",\"UK\",\"JP\"])].drop(columns={\"locale\"}).reset_index(drop=True)\n",
    "#test['last_item'] = test['prev_items'].apply(lambda x: str2list(x)[-1])\n",
    "#test['last_item'] = test['prev_items'].apply(lambda x: str2list(x)[-1])\n",
    "\n",
    "\n",
    "#total_items = set(df_next[\"item\"].unique())\n",
    "\n",
    "#len(set(test[\"last_item\"].unique()) - total_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "63465bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"sessions_test_task1.csv\")\n",
    "\n",
    "train = pd.read_csv(\"sessions_train.csv\")\n",
    "\n",
    "test_de = test[test[\"locale\"]==\"DE\"]\n",
    "test_uk = test[test[\"locale\"]==\"UK\"]\n",
    "test_jp = test[test[\"locale\"]==\"JP\"]\n",
    "\n",
    "\n",
    "test_de['last_item'] = test_de['prev_items'].apply(lambda x: str2list(x)[-1])\n",
    "test_uk['last_item'] = test_uk['prev_items'].apply(lambda x: str2list(x)[-1])\n",
    "test_jp['last_item'] = test_jp['prev_items'].apply(lambda x: str2list(x)[-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "58c4299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "489e304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"next_item_map.pkl\", \"rb\")\n",
    "next_item_map = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f3ace",
   "metadata": {},
   "source": [
    "DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "51b3ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_de[\"next_item_prediction1\"] = test_de[\"last_item\"].map(segundo_dic)\n",
    "test_de[\"next_item_prediction2\"] = test_de[\"last_item\"].map(next_item_map)\n",
    "test_de['next_item_prediction'] = test_de.next_item_prediction1.combine_first(test_de.next_item_prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda64d7",
   "metadata": {},
   "source": [
    "UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ebee525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"iknn_uk.pkl\", \"rb\")\n",
    "primer_dic_uk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9cf08e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segundo_dic_uk = {}\n",
    "\n",
    "for clave, valores in primer_dic_uk.items():\n",
    "    segundo_dic_uk[clave] = list(valores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "366e8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uk[\"next_item_prediction1\"] = test_uk[\"last_item\"].map(segundo_dic_uk)\n",
    "test_uk[\"next_item_prediction2\"] = test_uk[\"last_item\"].map(next_item_map)\n",
    "test_uk['next_item_prediction'] = test_uk.next_item_prediction1.combine_first(test_uk.next_item_prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d14377",
   "metadata": {},
   "source": [
    "JP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "57f6b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"iknn_jp.pkl\", \"rb\")\n",
    "primer_dic_jp = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "137481f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "segundo_dic_jp = {}\n",
    "\n",
    "for clave, valores in primer_dic_jp.items():\n",
    "    segundo_dic_jp[clave] = list(valores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ae12f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jp[\"next_item_prediction1\"] = test_jp[\"last_item\"].map(segundo_dic_jp)\n",
    "test_jp[\"next_item_prediction2\"] = test_jp[\"last_item\"].map(next_item_map)\n",
    "test_jp['next_item_prediction'] = test_jp.next_item_prediction1.combine_first(test_jp.next_item_prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f8e50",
   "metadata": {},
   "source": [
    "BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "03402cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deba6570bf04cacaa2851c13e3da1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3606249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964c129707bb43358994a03fdabe3be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8809a3f6294ddb9e98c496d9328051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1334818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import gc\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# df_prod = pd.read_csv('data/products_train.csv')\n",
    "# df_prod\n",
    "\n",
    "df_sess = pd.read_csv('sessions_train.csv')\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('sessions_test_task1.csv')\n",
    "\n",
    "\n",
    "def str2list(x):\n",
    "    x = x.replace('[', '').replace(']', '').replace(\"'\", '').replace('\\n', ' ').replace('\\r', ' ')\n",
    "    l = [i for i in x.split() if i]\n",
    "    return l\n",
    "\n",
    "next_item_dict = defaultdict(list)\n",
    "\n",
    "for _, row in tqdm(df_sess.iterrows(), total=len(df_sess)):\n",
    "    prev_items = str2list(row['prev_items'])\n",
    "    next_item = row['next_item']\n",
    "    prev_items_length = len(prev_items)\n",
    "    if prev_items_length <= 1:\n",
    "        next_item_dict[prev_items[0]].append(next_item)\n",
    "    else:\n",
    "        for i, item in enumerate(prev_items[:-1]):\n",
    "            next_item_dict[item].append(prev_items[i+1])\n",
    "        next_item_dict[prev_items[-1]].append(next_item)\n",
    "\n",
    "for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    prev_items = str2list(row['prev_items'])\n",
    "    prev_items_length = len(prev_items)\n",
    "    if prev_items_length <= 1:\n",
    "        continue\n",
    "    else:\n",
    "        for i, item in enumerate(prev_items[:-1]):\n",
    "            next_item_dict[item].append(prev_items[i+1])\n",
    "\n",
    "next_item_map = {}\n",
    "\n",
    "for item in tqdm(next_item_dict):\n",
    "    counter = Counter(next_item_dict[item])\n",
    "    next_item_map[item] = [i[0] for i in counter.most_common(100)]\n",
    "\n",
    "k = []\n",
    "v = []\n",
    "\n",
    "for item in next_item_dict:\n",
    "    k.append(item)\n",
    "    v.append(next_item_dict[item])\n",
    "    \n",
    "df_next = pd.DataFrame({'item': k, 'next_item': v})\n",
    "df_next = df_next.explode('next_item').reset_index(drop=True)\n",
    "\n",
    "\n",
    "top200 = df_next['next_item'].value_counts().index.tolist()[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf6858",
   "metadata": {},
   "source": [
    "JUNTAMOS TODO EN test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "757c80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([test_de[[\"locale\",\"prev_items\",\"next_item_prediction\"]], \n",
    "                     test_jp[[\"locale\",\"prev_items\",\"next_item_prediction\"]],\n",
    "                     test_uk[[\"locale\",\"prev_items\",\"next_item_prediction\"]]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "3a04b419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B08V12CT4C' 'B08V1KXBQD' 'B01BVG1XJS' 'B09VC...</td>\n",
       "      <td>[B099NR3X6D, B07LG5T3V9, B08496TCCQ, B099NS1XP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B00R9R5ND6' 'B00R9RZ9ZS' 'B00R9RZ9ZS']</td>\n",
       "      <td>[B00R9R5ND6, B004ZXMV4Q, B097HPKM63, B08NJP33W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B07YSRXJD3' 'B07G7Q5N6G' 'B08C9Q7QVK' 'B07G7...</td>\n",
       "      <td>[B08C9Q7QVK, B0B5QNFWJ1, B0BBVB89CS, B0B5TFLBC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B08KQBYV43' '3955350843' '3955350843' '39553...</td>\n",
       "      <td>[3772476953, 395535086X, 3955350878, 377247791...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B09FPTCWMC' 'B09FPTQP68' 'B08HMRY8NG' 'B08TB...</td>\n",
       "      <td>[B09J8V18FL, B09J8T6TTH, B09J8SKX9G, B09J8VPTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316966</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B077SZ2C3Y' 'B0B14M3VZX']</td>\n",
       "      <td>[B08X9L5RGD, B07BKX8KH7, B09SV27FHD, B07LB4YTH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B08KFHDPY9' 'B0851KTSRZ' 'B08KFHDPY9' 'B0851...</td>\n",
       "      <td>[B09CPNS7XV, B0989BHLSY, B09895QPQF, B09CPP92Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B07PY1N81F' 'B07Q1Z8SQN' 'B07PY1N81F' 'B07Q1...</td>\n",
       "      <td>[B09HKZBNZH, B07PY1N81F, B07PY1NG3X, B09HZSRJW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B01MCQMORK' 'B09JYZ325W']</td>\n",
       "      <td>[B07TR5LQSL, B08FB464L7, B005G3DI32, B01MCQMOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B0B8JX92YJ' 'B09TN4MP6V' 'B0BG2LZQSL']</td>\n",
       "      <td>[B09XPX59JK, B09TN4MP6V, B098BHY4X2, B098F363M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316971 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                                         prev_items  \\\n",
       "0          DE  ['B08V12CT4C' 'B08V1KXBQD' 'B01BVG1XJS' 'B09VC...   \n",
       "1          DE           ['B00R9R5ND6' 'B00R9RZ9ZS' 'B00R9RZ9ZS']   \n",
       "2          DE  ['B07YSRXJD3' 'B07G7Q5N6G' 'B08C9Q7QVK' 'B07G7...   \n",
       "3          DE  ['B08KQBYV43' '3955350843' '3955350843' '39553...   \n",
       "4          DE  ['B09FPTCWMC' 'B09FPTQP68' 'B08HMRY8NG' 'B08TB...   \n",
       "...       ...                                                ...   \n",
       "316966     UK                        ['B077SZ2C3Y' 'B0B14M3VZX']   \n",
       "316967     UK  ['B08KFHDPY9' 'B0851KTSRZ' 'B08KFHDPY9' 'B0851...   \n",
       "316968     UK  ['B07PY1N81F' 'B07Q1Z8SQN' 'B07PY1N81F' 'B07Q1...   \n",
       "316969     UK                        ['B01MCQMORK' 'B09JYZ325W']   \n",
       "316970     UK           ['B0B8JX92YJ' 'B09TN4MP6V' 'B0BG2LZQSL']   \n",
       "\n",
       "                                     next_item_prediction  \n",
       "0       [B099NR3X6D, B07LG5T3V9, B08496TCCQ, B099NS1XP...  \n",
       "1       [B00R9R5ND6, B004ZXMV4Q, B097HPKM63, B08NJP33W...  \n",
       "2       [B08C9Q7QVK, B0B5QNFWJ1, B0BBVB89CS, B0B5TFLBC...  \n",
       "3       [3772476953, 395535086X, 3955350878, 377247791...  \n",
       "4       [B09J8V18FL, B09J8T6TTH, B09J8SKX9G, B09J8VPTT...  \n",
       "...                                                   ...  \n",
       "316966  [B08X9L5RGD, B07BKX8KH7, B09SV27FHD, B07LB4YTH...  \n",
       "316967  [B09CPNS7XV, B0989BHLSY, B09895QPQF, B09CPP92Q...  \n",
       "316968  [B09HKZBNZH, B07PY1N81F, B07PY1NG3X, B09HZSRJW...  \n",
       "316969  [B07TR5LQSL, B08FB464L7, B005G3DI32, B01MCQMOR...  \n",
       "316970  [B09XPX59JK, B09TN4MP6V, B098BHY4X2, B098F363M...  \n",
       "\n",
       "[316971 rows x 3 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbc944",
   "metadata": {},
   "source": [
    "ARREGLO PREDS DEL BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "393ac642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28586a1b7de4bedb38cd2bafa2e3549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    pred_orig = row['next_item_prediction']\n",
    "    pred = pred_orig\n",
    "    prev_items = str2list(row['prev_items'])\n",
    "    if type(pred) == float:\n",
    "        pred = top200[:100]\n",
    "    else:\n",
    "        if len(pred_orig) < 100:\n",
    "            for i in top200:\n",
    "                if i not in pred_orig and i not in prev_items:\n",
    "                    pred.append(i)\n",
    "                if len(pred) >= 100:\n",
    "                    break\n",
    "        else:\n",
    "            pred = pred[:100]\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1ed3520b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B08V12CT4C' 'B08V1KXBQD' 'B01BVG1XJS' 'B09VC...</td>\n",
       "      <td>[B099NR3X6D, B07LG5T3V9, B08496TCCQ, B099NS1XP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B00R9R5ND6' 'B00R9RZ9ZS' 'B00R9RZ9ZS']</td>\n",
       "      <td>[B00R9R5ND6, B004ZXMV4Q, B097HPKM63, B08NJP33W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B07YSRXJD3' 'B07G7Q5N6G' 'B08C9Q7QVK' 'B07G7...</td>\n",
       "      <td>[B08C9Q7QVK, B0B5QNFWJ1, B0BBVB89CS, B0B5TFLBC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B08KQBYV43' '3955350843' '3955350843' '39553...</td>\n",
       "      <td>[3772476953, 395535086X, 3955350878, 377247791...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>['B09FPTCWMC' 'B09FPTQP68' 'B08HMRY8NG' 'B08TB...</td>\n",
       "      <td>[B09J8V18FL, B09J8T6TTH, B09J8SKX9G, B09J8VPTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316966</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B077SZ2C3Y' 'B0B14M3VZX']</td>\n",
       "      <td>[B08X9L5RGD, B07BKX8KH7, B09SV27FHD, B07LB4YTH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B08KFHDPY9' 'B0851KTSRZ' 'B08KFHDPY9' 'B0851...</td>\n",
       "      <td>[B09CPNS7XV, B0989BHLSY, B09895QPQF, B09CPP92Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B07PY1N81F' 'B07Q1Z8SQN' 'B07PY1N81F' 'B07Q1...</td>\n",
       "      <td>[B09HKZBNZH, B07PY1N81F, B07PY1NG3X, B09HZSRJW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B01MCQMORK' 'B09JYZ325W']</td>\n",
       "      <td>[B07TR5LQSL, B08FB464L7, B005G3DI32, B01MCQMOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>['B0B8JX92YJ' 'B09TN4MP6V' 'B0BG2LZQSL']</td>\n",
       "      <td>[B09XPX59JK, B09TN4MP6V, B098BHY4X2, B098F363M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316971 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                                         prev_items  \\\n",
       "0          DE  ['B08V12CT4C' 'B08V1KXBQD' 'B01BVG1XJS' 'B09VC...   \n",
       "1          DE           ['B00R9R5ND6' 'B00R9RZ9ZS' 'B00R9RZ9ZS']   \n",
       "2          DE  ['B07YSRXJD3' 'B07G7Q5N6G' 'B08C9Q7QVK' 'B07G7...   \n",
       "3          DE  ['B08KQBYV43' '3955350843' '3955350843' '39553...   \n",
       "4          DE  ['B09FPTCWMC' 'B09FPTQP68' 'B08HMRY8NG' 'B08TB...   \n",
       "...       ...                                                ...   \n",
       "316966     UK                        ['B077SZ2C3Y' 'B0B14M3VZX']   \n",
       "316967     UK  ['B08KFHDPY9' 'B0851KTSRZ' 'B08KFHDPY9' 'B0851...   \n",
       "316968     UK  ['B07PY1N81F' 'B07Q1Z8SQN' 'B07PY1N81F' 'B07Q1...   \n",
       "316969     UK                        ['B01MCQMORK' 'B09JYZ325W']   \n",
       "316970     UK           ['B0B8JX92YJ' 'B09TN4MP6V' 'B0BG2LZQSL']   \n",
       "\n",
       "                                     next_item_prediction  \n",
       "0       [B099NR3X6D, B07LG5T3V9, B08496TCCQ, B099NS1XP...  \n",
       "1       [B00R9R5ND6, B004ZXMV4Q, B097HPKM63, B08NJP33W...  \n",
       "2       [B08C9Q7QVK, B0B5QNFWJ1, B0BBVB89CS, B0B5TFLBC...  \n",
       "3       [3772476953, 395535086X, 3955350878, 377247791...  \n",
       "4       [B09J8V18FL, B09J8T6TTH, B09J8SKX9G, B09J8VPTT...  \n",
       "...                                                   ...  \n",
       "316966  [B08X9L5RGD, B07BKX8KH7, B09SV27FHD, B07LB4YTH...  \n",
       "316967  [B09CPNS7XV, B0989BHLSY, B09895QPQF, B09CPP92Q...  \n",
       "316968  [B09HKZBNZH, B07PY1N81F, B07PY1NG3X, B09HZSRJW...  \n",
       "316969  [B07TR5LQSL, B08FB464L7, B005G3DI32, B01MCQMOR...  \n",
       "316970  [B09XPX59JK, B09TN4MP6V, B098BHY4X2, B098F363M...  \n",
       "\n",
       "[316971 rows x 3 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['next_item_prediction'] = preds\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0b8a9631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    316971.0\n",
       "mean        100.0\n",
       "std           0.0\n",
       "min         100.0\n",
       "25%         100.0\n",
       "50%         100.0\n",
       "75%         100.0\n",
       "max         100.0\n",
       "Name: next_item_prediction, dtype: float64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['next_item_prediction'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d4b154ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['locale', 'next_item_prediction']].to_parquet('submission_task1.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "59c4a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sessions=pd.read_csv('sessions_test_task1.csv')\n",
    "def check_predictions(predictions, check_products=False):\n",
    "    \"\"\"\n",
    "    These tests need to pass as they will also be applied on the evaluator\n",
    "    \"\"\"\n",
    "    test_locale_names = test_sessions['locale'].unique()\n",
    "    for locale in test_locale_names:\n",
    "        sess_test = test_sessions.query(f'locale == \"{locale}\"')\n",
    "        preds_locale =  predictions[predictions['locale'] == sess_test['locale'].iloc[0]]\n",
    "        assert sorted(preds_locale.index.values) == sorted(sess_test.index.values), f\"Session ids of {locale} doesn't match\"\n",
    "\n",
    "        if check_products:\n",
    "            # This check is not done on the evaluator\n",
    "            # but you can run it to verify there is no mixing of products between locales\n",
    "            # Since the ground truth next item will always belong to the same locale\n",
    "            # Warning - This can be slow to run\n",
    "            products = read_product_data().query(f'locale == \"{locale}\"')\n",
    "            predicted_products = np.unique( np.array(list(preds_locale[\"next_item_prediction\"].values)) )\n",
    "            assert np.all( np.isin(predicted_products, products['id']) ), f\"Invalid products in {locale} predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c21f4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_predictions(df_test[['locale', 'next_item_prediction']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe204ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
